# NER11 TASKMASTER: Training Phase
**Mission**: Configure and Execute NER11 Model Training
**Date**: 2025-11-25
**Status**: READY FOR TRAINING
**Phase**: 3 - Configuration & Training

---

## Current Status Summary

### ‚úÖ COMPLETED PHASES

**Phase 1: Foundation** ‚úÖ
- [x] Directory structure initialized
- [x] Reference documentation populated (566 entity schema)
- [x] Custom data collected (1,945 files)
- [x] Docker environment built and verified

**Phase 2: Data Preparation** ‚úÖ
- [x] External datasets acquired (9 datasets)
- [x] Schema harmonization completed (all datasets ‚Üí NER11 schema)
- [x] Custom data conversion (auto-labeling with entity master list)
- [x] Data weighting applied (51 Gold Standard files at 3.0x)
- [x] **Dataset merge completed** (76,267 total documents)

### üìä Final Dataset Statistics
- **Training Documents**: 67,230
- **Development Documents**: 9,037
- **Total Entities**: 306,053
- **Active Entity Types**: 61 (from 566 available)
- **Files**: `final_training_set/train.spacy` (44MB), `dev.spacy` (8.1MB)

---

## üéØ CURRENT PHASE: Training Configuration & Execution

### Task 3.1: Generate Training Configuration ‚è≥ NEXT
**Owner**: ML_ENGINEER
**Objective**: Create optimized `config.cfg` for transformer-based NER training

**Requirements**:
1. **Architecture**: Transformer-based (en_core_web_trf)
2. **GPU**: CUDA 11.8 support, GPU ID 0
3. **Memory**: Optimized for 50-60GB RAM, 16GB shared memory
4. **Batch Size**: Start with 128, adjust based on GPU memory (8GB VRAM)
5. **Learning Rate**: 1e-5 to 5e-5 (transformer fine-tuning range)
6. **Epochs**: 10-20 with early stopping
7. **Dropout**: 0.1-0.2
8. **Mixed Precision**: Enable FP16 for faster training

**Deliverable**: `env/config.cfg`

**Command to Generate Base Config**:
```bash
docker exec ner11_training_env python3 -m spacy init config \
  /app/NER11_Gold_Standard/env/base_config.cfg \
  --lang en \
  --pipeline ner \
  --optimize efficiency \
  --gpu
```

**Required Modifications**:
- Set `gpu_allocator = "pytorch"`
- Configure paths to `final_training_set/train.spacy` and `dev.spacy`
- Adjust batch size based on GPU memory
- Enable transformer components
- Set appropriate learning rate schedule

---

### Task 3.2: Execute Training ‚è∏Ô∏è PENDING
**Owner**: ML_ENGINEER
**Objective**: Train NER11 model with GPU acceleration

**Prerequisites**:
- [x] Docker container running (`ner11_training_env`)
- [x] Training data merged (`final_training_set/`)
- [ ] Training configuration created (`env/config.cfg`)

**Command**:
```bash
docker exec ner11_training_env python3 -m spacy train \
  /app/NER11_Gold_Standard/env/config.cfg \
  --output /app/NER11_Gold_Standard/models/ner11_v1 \
  --paths.train /app/NER11_Gold_Standard/final_training_set/train.spacy \
  --paths.dev /app/NER11_Gold_Standard/final_training_set/dev.spacy \
  --gpu-id 0
```

**Expected Duration**: 2-6 hours (depending on epochs and batch size)

**Monitoring**:
- Watch F1 scores on dev set
- Monitor loss convergence
- Check GPU utilization (should be >90%)
- Watch for overfitting (dev F1 decreasing while train F1 increasing)

**Success Criteria**:
- Training completes without OOM errors
- Loss converges
- Dev F1 score > 0.75 (target: 0.80-0.85)
- Model saved to `models/ner11_v1/`

---

### Task 3.3: Create Training Script (Optional) ‚è∏Ô∏è PENDING
**Owner**: ML_ENGINEER
**Objective**: Create reusable training script with logging

**Deliverable**: `scripts/train_ner11.sh`

**Features**:
- Automated training execution
- Progress logging to file
- GPU monitoring
- Automatic model versioning
- Email/notification on completion

---

## üìã Phase 4: Validation & Deployment (UPCOMING)

### Task 4.1: Evaluate Model
**Owner**: QA_OFFICER
**Objective**: Calculate comprehensive evaluation metrics

**Metrics to Calculate**:
- Overall F1, Precision, Recall
- Per-entity-type F1 scores
- Confusion matrix
- Error analysis (false positives/negatives)

**Quality Gates**:
- Overall F1 > 0.80
- Critical entity types (CRITICAL_INFRASTRUCTURE, CYBER_THREAT, CVE, DEVICE) F1 > 0.85
- No catastrophic failures on any entity type

### Task 4.2: Package Model
**Owner**: DATA_OPS
**Objective**: Package model for deployment

**Deliverables**:
- Model export to portable format
- Inference API wrapper
- Docker container for deployment
- Documentation and usage examples

### Task 4.3: Deploy to Production
**Owner**: DATA_OPS
**Objective**: Deploy NER11 to AEON Cyber Digital Twin

**Integration Points**:
- AEON ingestion pipeline
- Real-time entity extraction
- Knowledge graph population
- Monitoring and alerting

---

## üîß Technical Environment

### Docker Container Status
- **Container**: `ner11_training_env` ‚úÖ RUNNING
- **Image**: `ner11-gold-standard:latest` (29.3GB)
- **GPU**: NVIDIA GeForce RTX 4060 (8GB VRAM) ‚úÖ ACCESSIBLE
- **Memory**: 16GB shared memory allocated
- **Python**: 3.10
- **spaCy**: 3.8.11
- **PyTorch**: 2.9.1+cu128
- **CUDA**: 11.8.0

### Key Directories
```
/app/NER11_Gold_Standard/
‚îú‚îÄ‚îÄ final_training_set/     # Merged training data ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ train.spacy         # 67,230 docs
‚îÇ   ‚îî‚îÄ‚îÄ dev.spacy           # 9,037 docs
‚îú‚îÄ‚îÄ env/                    # Configuration files
‚îÇ   ‚îî‚îÄ‚îÄ config.cfg          # ‚è∏Ô∏è TO BE CREATED
‚îú‚îÄ‚îÄ models/                 # Trained models output
‚îÇ   ‚îî‚îÄ‚îÄ ner11_v1/          # ‚è∏Ô∏è TO BE CREATED
‚îú‚îÄ‚îÄ scripts/               # Training scripts
‚îî‚îÄ‚îÄ reference/             # Schema documentation
```

---

## üìä Success Metrics

### Training Metrics (Target)
- **F1 Score**: > 0.80 (target: 0.85)
- **Precision**: > 0.82
- **Recall**: > 0.78
- **Training Time**: < 8 hours
- **GPU Utilization**: > 90%

### Entity-Specific Targets
| Entity Type | Target F1 | Priority |
|-------------|-----------|----------|
| CRITICAL_INFRASTRUCTURE | > 0.85 | High |
| CYBER_THREAT | > 0.85 | High |
| CVE | > 0.90 | High |
| DEVICE | > 0.85 | High |
| MEASUREMENT | > 0.80 | Medium |
| CONTROLS | > 0.80 | Medium |

---

## üö® Risk Mitigation

### Known Risks
1. **OOM Errors**: 67k documents may strain 8GB GPU
   - **Mitigation**: Start with batch size 64, use gradient accumulation
   
2. **Overfitting**: Large dataset with 61 entity types
   - **Mitigation**: Use dropout 0.2, early stopping, monitor dev F1
   
3. **Class Imbalance**: Top 20 entities = 80% of data
   - **Mitigation**: Consider class weights or focal loss
   
4. **Training Time**: May take 4-8 hours
   - **Mitigation**: Use mixed precision (FP16), optimize batch size

---

## üìù Next Immediate Action

**STEP 1**: Generate base training configuration
```bash
docker exec ner11_training_env python3 -m spacy init config \
  /app/NER11_Gold_Standard/env/base_config.cfg \
  --lang en \
  --pipeline ner \
  --optimize efficiency \
  --gpu
```

**STEP 2**: Review and modify config for NER11 requirements

**STEP 3**: Execute training

**STEP 4**: Monitor and evaluate

---

## üìö Reference Documents
- **Merge Completion Report**: `walkthrough.md` (in artifacts)
- **Entity Schema**: `reference/NER11_ENTITY_MASTER_LIST.md`
- **Dataset Statistics**: See "Final Dataset Statistics" above
- **Docker Setup**: `docker/Dockerfile`, `docker/docker-compose.yml`
